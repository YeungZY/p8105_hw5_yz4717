---
title: "p8105_hw5_yz4717"
author: "Yang Zhao - yz4717"
date: "2023-11-12"
output: 
  github_document:
    toc: TRUE
---

```{r}
library(tidyverse)
library(ggplot2)

set.seed(1)
```

# Question 1

```{r}
homicide_data_raw = read_csv("homicide-data.csv")
```

The raw data consist `r ncol(homicide_data_raw)` variables and `r nrow(homicide_data_raw)` observations. Each observation includes uid, reported date, victim first and last name, race, age, sex, city, state, lat, lon, and disposition.

```{r}
homicide_data = 
homicide_data_raw |>
  mutate(city_state = paste(city, state, sep=", "),
         disposition = case_match(
           disposition,
           "Closed by arrest" ~ "homicides",
           "Closed without arrest" ~ "unsolved homicides",
           "Open/No arrest" ~ "unsolved homicides"
         )) |>
  group_by(city_state, disposition) |>
  summarize(n = n()) |>
  filter(city_state != "Tulsa, AL") 
```

Tulsa, AL in city_state is removed, as it is not a major US city.

```{r}
# prop.test
homicide_data_Bal = homicide_data |>
  pivot_wider(
    names_from = "disposition", 
    values_from = "n") |>
  filter(city_state == "Baltimore, MD")

prop_test_Bal = 
  prop.test(pull(homicide_data_Bal, `unsolved homicides`), 
            pull(homicide_data_Bal, homicides) + pull(homicide_data_Bal,`unsolved homicides`))

prop_test_Bal |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high)
```


```{r}
# prop.test for each city
homicide_data_prop = homicide_data |>
  pivot_wider(
    names_from = "disposition", 
    values_from = "n") |>
    filter(city_state != "Tulsa, AL") 

prop_test_city_state = function(a) {
    x = homicide_data_prop |>
      filter(city_state == a)
    
    test = prop.test(pull(x, `unsolved homicides`),
                     pull(x, homicides) + pull(x,`unsolved homicides`))
    test_result = test |>
      broom::tidy() |>
      select(estimate, conf.low, conf.high)
    
    test_result
}

test_res  = 
  data.frame(city_state = unique(pull(homicide_data_prop, city_state))) |>
  mutate(prop_test_res = purrr::map(city_state,prop_test_city_state)) |>
  unnest(cols = c(prop_test_res))
```

plot of the estimates and CIs for each city

```{r}
test_res |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot arrange the estimate of the proportion of unsolved homicides in city,state. Chicago has the highest estimate and small range of confidence interval.

# Question 2

```{r q2_import, message=FALSE}

q2_filenames = 
  list.files(path = "data",pattern = ".csv")

q2_path = file.path("data",q2_filenames)
combined_data = NULL

data_q2 = tibble(
  source = q2_filenames,
  tibble = map(q2_path, read_csv)
) |> 
  unnest(tibble) |> 
  mutate(source = str_replace(source,".csv","")) |> 
  separate(source, into = c("arm","subject_id"), sep = "_") |> 
  relocate(arm,subject_id)

```

First, I listed all the file names via `list.files` and applied the map function to read all files in the founder. Besides, I added a variable `source` to track the data and made it into a reader friendly table.

```{r q2_cleaned}
data_q2 |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value"
  ) |> 
  mutate(week = str_replace(week,"week_",""),
         arm = case_match(arm,
                          "con" ~ "control",
                          "exp" ~ "experiment"))|> 
  ggplot(aes(x = week, y = value,
             group = subject_id, 
             color = subject_id))+
  geom_point()+
  geom_line()+
  labs(title = " Spaghetti Plot ",
       x = "Week",
       y = "Value",
       color = "Group")+
  facet_grid(~arm)
  
```

From the shown graph, in the control group, the data show some fluctuations, but on the whole, the data in the control group do not show any obvious upward or downward trend. However, within the experiment group, there is a obvious upward tendency. There is a significant difference between these two.

# Question 3

```{r q3_function}
mu = 0
sigma = 5
observations = 30
num_datasets = 5000

normtest = function(observations = 30,mu = 0,sigma = 5){
  sample = rnorm(n=observations,mean = mu,sd = sigma)
  result = t.test(sample,alternative = "two.sided",conf.level = .95) |> 
    broom::tidy() |> 
    select(estimate,p.value)
  return(result)
}

```

In the question 3, to avoid the redundancy, I wrote the simulation procedure that needed to be repeated, containing data cleaning, into a function called `normtest`. 

The function of `normtest` first generates `r observations` random numbers based on a normal distribution with `mean = 0` and `standard deviation = 5` , followed by a two-sided hypothesis test with a 95% confidence level on the resulting samples. Cleaning and integration of the results is also included in this function.

```{r q3_power_plot}
test_result = data_frame()
final_result = list(length = 7)

for (a in 0:6) {
  test_result = NULL
  for(i in 1:num_datasets){
  test_result = rbind(test_result,normtest(mu = a))
  }
  final_result[[a+1]] = test_result
}

power = as.vector(
  map(final_result, \(x) sum(x$p.value<.05)/5000) |> 
  unlist())
power_eff = tibble(power,effect_size = c(0:6))

power_eff |>
  ggplot(aes(x = effect_size, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Population Mean & Power",
       x = "Population Mean",
       y = "Power")

```

* comment: It is obvious that, as the population mean increasing, power rise rapidly. As the power approaching 1, the slower it grows. 

```{r}

mean_est = 
  as.vector(
  map(final_result,\(x) mean(x$estimate)) |> 
    unlist()
  )

ave_and_true = 
  data.frame(mean_est,mu = 0:6)

ave_and_true |>
  ggplot(aes(x = mu,
             y = mean_est)) +
  geom_point(color = "yellow") +
  geom_line(color = "yellow") +
  labs(title = "Average Estimate & Population Mean",
       x = "Population Mean",
       y = "Average Estimate") 

```

At the first, we have to draw a line of population mean.

```{r q3_}
reject_true = lapply(final_result,function(df){
   filtered = df |> 
     filter(df$p.value<.05)
   return(mean(filtered$estimate))
}) |> 
  unlist()
  
rej_true_df = 
  data.frame(reject_true,
             true_mean = 0:6)

ggplot() +
  geom_point(data = ave_and_true,
             aes(x = mu, y = mean_est),
             color = "yellow") +
  geom_line(data = ave_and_true,
            aes(x = mu, y = mean_est),
            color = "yellow") +
  geom_point(data = rej_true_df,
             aes(x = true_mean, y = reject_true),
             color = "blue") +
  geom_line(data = rej_true_df,
            aes(x = true_mean, y = reject_true),
            color = "blue") +
  labs(title = "Average Estimate & Population Mean",
       x = "Population Mean",
       y = "Average Estimate")
```

* Comment: The yellow line represents the average estimate compared to the population mean, indicating a close alignment between the two. On the other hand, the blue line depicts the average estimate when the null hypothesis is rejected versus the population mean. Notably, for null rejections corresponding to mean values of 0, 1, 2, 3, 4, 5 and 6, the sample average does not closely approximate the population mean. This discrepancy is attributed to insufficient statistical power. The sample average estimate converges with the mean as statistical power or effect size increases.
